{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import torch\n",
                "import numpy as np\n",
                "import random\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import models, transforms\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomDataset(Dataset):\n",
                "    def __init__(self, imgs_folder, labels_txt):\n",
                "        self.transform = transforms.Compose([\n",
                "            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "        ])\n",
                "        self.dataset = []\n",
                "        with open(labels_txt, 'r') as f:\n",
                "            for line in f.readlines():\n",
                "                img_file, label = line.split(\",\")\n",
                "                img_file = os.path.join(imgs_folder, img_file)\n",
                "                label = int(label)\n",
                "                self.dataset.append((img_file, label))\n",
                "\n",
                "        random.shuffle(self.dataset)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataset)\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        img_path, label = self.dataset[index]\n",
                "        img = cv2.imread(img_path)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
                "\n",
                "        img = self.transform(img)\n",
                "        img = img.to(torch.float32)\n",
                "\n",
                "        label = torch.tensor(label, dtype=torch.float32)\n",
                "        return img, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_path = \"/media/work/matheusvieira/truck_axle/Dataset/\"\n",
                "train_dataset = CustomDataset(dataset_path + \"train/images\", dataset_path + \"train/number_of_axles_train.txt\")\n",
                "valid_dataset = CustomDataset(dataset_path + \"valid/images\", dataset_path + \"valid/number_of_axles_valid.txt\")\n",
                "test_dataset = CustomDataset(dataset_path + \"test/images\", dataset_path + \"test/number_of_axles_test.txt\")\n",
                "\n",
                "BATCH_SIZE = 8\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
                "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SequentialModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(SequentialModel, self).__init__()\n",
                "        self.model = models.resnet152(weights=(models.ResNet152_Weights.IMAGENET1K_V1))\n",
                "\n",
                "        self.model.fc = nn.Sequential(\n",
                "            nn.Linear(2048, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(512, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Linear(256, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.model(x)\n",
                "        return x\n",
                "\n",
                "# Instantiate the model\n",
                "model = SequentialModel()\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1, Train Loss: 2.3463, Train MAE: 1.1166, Val Loss: 0.5995, Val MAE Loss: 0.6330\n",
                        "Best model saved at epoch 1\n",
                        "Epoch 2, Train Loss: 0.9273, Train MAE: 0.7113, Val Loss: 0.3362, Val MAE Loss: 0.4113\n",
                        "Best model saved at epoch 2\n",
                        "Epoch 3, Train Loss: 0.7539, Train MAE: 0.6271, Val Loss: 0.5334, Val MAE Loss: 0.5536\n",
                        "Epoch 4, Train Loss: 0.5425, Train MAE: 0.5370, Val Loss: 0.3325, Val MAE Loss: 0.3904\n",
                        "Best model saved at epoch 4\n",
                        "Epoch 5, Train Loss: 0.3801, Train MAE: 0.4524, Val Loss: 0.7028, Val MAE Loss: 0.6176\n",
                        "Epoch 6, Train Loss: 0.2939, Train MAE: 0.3920, Val Loss: 0.3519, Val MAE Loss: 0.4345\n",
                        "Epoch 7, Train Loss: 0.2433, Train MAE: 0.3590, Val Loss: 0.3257, Val MAE Loss: 0.4032\n",
                        "Best model saved at epoch 7\n",
                        "Epoch 8, Train Loss: 0.2764, Train MAE: 0.3794, Val Loss: 0.1255, Val MAE Loss: 0.2664\n",
                        "Best model saved at epoch 8\n",
                        "Epoch 9, Train Loss: 0.1674, Train MAE: 0.3003, Val Loss: 0.1144, Val MAE Loss: 0.2409\n",
                        "Best model saved at epoch 9\n",
                        "Epoch 10, Train Loss: 0.1826, Train MAE: 0.3021, Val Loss: 0.2419, Val MAE Loss: 0.3328\n",
                        "Epoch 11, Train Loss: 0.1489, Train MAE: 0.2806, Val Loss: 0.2006, Val MAE Loss: 0.3246\n",
                        "Epoch 12, Train Loss: 0.1125, Train MAE: 0.2496, Val Loss: 0.1073, Val MAE Loss: 0.2317\n",
                        "Best model saved at epoch 12\n",
                        "Epoch 13, Train Loss: 0.1400, Train MAE: 0.2656, Val Loss: 0.2243, Val MAE Loss: 0.3030\n",
                        "Epoch 14, Train Loss: 0.1287, Train MAE: 0.2608, Val Loss: 0.2571, Val MAE Loss: 0.3190\n",
                        "Epoch 15, Train Loss: 0.1109, Train MAE: 0.2391, Val Loss: 0.3148, Val MAE Loss: 0.3427\n",
                        "Early stopping\n"
                    ]
                }
            ],
            "source": [
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
                "MAE = nn.L1Loss()\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "patience = 3\n",
                "counter = 0\n",
                "\n",
                "for epoch in range(25):\n",
                "    model.train()\n",
                "    train_loss = 0.0\n",
                "    mae_train_loss = 0.0\n",
                "\n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs.squeeze(), labels)\n",
                "        mae_loss = MAE(outputs.squeeze(), labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        train_loss += loss.item() * images.size(0)\n",
                "        mae_train_loss += mae_loss.item() * images.size(0)\n",
                "\n",
                "    train_loss /= len(train_loader.dataset)\n",
                "    mae_train_loss /= len(train_loader.dataset)\n",
                "\n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0.0\n",
                "    mae_val_loss = 0.0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in valid_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs.squeeze(), labels)\n",
                "            mae_loss = MAE(outputs.squeeze(), labels)\n",
                "\n",
                "            val_loss += loss.item() * images.size(0)\n",
                "            mae_val_loss += mae_loss.item() * images.size(0)\n",
                "\n",
                "    val_loss /= len(valid_loader.dataset)\n",
                "    mae_val_loss /= len(valid_loader.dataset)\n",
                "\n",
                "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train MAE: {mae_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MAE Loss: {mae_val_loss:.4f}\")\n",
                "\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        counter = 0\n",
                "        torch.save(model.state_dict(), f\"best_resnet.pth\")\n",
                "        print(f\"Best model saved at epoch {epoch+1}\")\n",
                "    else:\n",
                "        counter += 1\n",
                "        if counter >= patience:\n",
                "            print(\"Early stopping\")\n",
                "            model.load_state_dict(torch.load(f\"best_resnet.pth\"))\n",
                "            break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Loss: 0.0983 - Test MAE Loss: 0.2354\n"
                    ]
                }
            ],
            "source": [
                "model.eval()\n",
                "test_loss = 0.0\n",
                "test_mae_loss = 0.0\n",
                "\n",
                "with torch.no_grad():\n",
                "    for images, labels in test_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs.squeeze(), labels)\n",
                "        mae = MAE(outputs.squeeze(), labels)\n",
                "\n",
                "        test_loss += loss.item() * images.size(0)\n",
                "        test_mae_loss += mae.item() * images.size(0)\n",
                "\n",
                "test_loss /= len(test_loader.dataset)\n",
                "test_mae_loss /= len(test_loader.dataset)\n",
                "\n",
                "print(f\"Test Loss: {test_loss:.4f} - Test MAE Loss: {test_mae_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_prediction(model, img_path):\n",
                "    img = cv2.imread(img_path)\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
                "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "    img = transform(img)\n",
                "    img = img.to(torch.float32)\n",
                "    img = img.to(device)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        outputs = model(img.unsqueeze(0))\n",
                "        return outputs[0][0].item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "7: 6 - 6.328375816345215\n",
                        "3: 4 - 3.5770022869110107\n",
                        "3: 4 - 3.5590291023254395\n",
                        "3: 4 - 3.524505615234375\n",
                        "3: 4 - 3.649664878845215\n",
                        "3: 2 - 2.169879198074341\n",
                        "4: 3 - 3.399392604827881\n",
                        "6: 5 - 4.894862651824951\n",
                        "6: 5 - 5.385087490081787\n",
                        "4: 3 - 2.83590030670166\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "y_true = []\n",
                "y_pred = []\n",
                "\n",
                "with open(dataset_path+\"test/number_of_axles_test.txt\", 'r') as f:\n",
                "    for i in f.readlines():\n",
                "        img_file, label = i.split(\",\")\n",
                "        y_true.append(int(label))\n",
                "\n",
                "        img_file = os.path.join(dataset_path+\"/test/images\", img_file)\n",
                "\n",
                "        no_round = make_prediction(model, img_file)\n",
                "        predicao = round(no_round)\n",
                "        if predicao != int(label):\n",
                "            print(f\"{int(label)}: {predicao} - {no_round}\")\n",
                "        y_pred.append(predicao)\n",
                "\n",
                "confusion_matrix(y_true, y_pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(0.853077478077478, 0.8661522874288831, 0.854937154348919, None)\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "print(precision_recall_fscore_support(y_true, y_pred, average=\"macro\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.908256880733945"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "print(accuracy_score(y_true, y_pred))"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
