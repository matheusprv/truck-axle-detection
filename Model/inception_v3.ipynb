{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import cv2\n",
                "import torch\n",
                "import numpy as np\n",
                "import random\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import models, transforms\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class CustomDataset(Dataset):\n",
                "    def __init__(self, imgs_folder, labels_txt):\n",
                "        self.transform = transforms.Compose([\n",
                "            transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "        ])\n",
                "        self.dataset = []\n",
                "        with open(labels_txt, 'r') as f:\n",
                "            for line in f.readlines():\n",
                "                img_file, label = line.split(\",\")\n",
                "                img_file = os.path.join(imgs_folder, img_file)\n",
                "                label = int(label)\n",
                "                self.dataset.append((img_file, label))\n",
                "\n",
                "        random.shuffle(self.dataset)\n",
                "\n",
                "    def __len__(self):\n",
                "        return len(self.dataset)\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        img_path, label = self.dataset[index]\n",
                "        img = cv2.imread(img_path)\n",
                "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
                "\n",
                "        img = self.transform(img)\n",
                "        img = img.to(torch.float32)\n",
                "\n",
                "        label = torch.tensor(label, dtype=torch.float32)\n",
                "        return img, label"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataset_path = \"/media/work/matheusvieira/truck_axle/Dataset/\"\n",
                "train_dataset = CustomDataset(dataset_path + \"train/images\", dataset_path + \"train/number_of_axles_train.txt\")\n",
                "valid_dataset = CustomDataset(dataset_path + \"valid/images\", dataset_path + \"valid/number_of_axles_valid.txt\")\n",
                "test_dataset = CustomDataset(dataset_path + \"test/images\", dataset_path + \"test/number_of_axles_test.txt\")\n",
                "\n",
                "BATCH_SIZE = 8\n",
                "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
                "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class SequentialModel(nn.Module):\n",
                "    def __init__(self):\n",
                "        super(SequentialModel, self).__init__()\n",
                "\n",
                "        self.model = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
                "        self.model.aux_logits = False\n",
                "\n",
                "        self.model.fc = nn.Sequential(\n",
                "            nn.Linear(2048, 512),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(512, 256),\n",
                "            nn.ReLU(),\n",
                "            nn.BatchNorm1d(256),\n",
                "            nn.Linear(256, 32),\n",
                "            nn.ReLU(),\n",
                "            nn.Linear(32, 1)\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        x = self.model(x)\n",
                "        return x\n",
                "\n",
                "# Instantiate the model\n",
                "model = SequentialModel()\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "model = model.to(device)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Epoch 1, Train Loss: 3.1897, Train MAE: 1.3277, Val Loss: 1.4656, Val MAE Loss: 1.0141\n",
                        "Best model saved at epoch 1\n",
                        "Epoch 2, Train Loss: 1.1564, Train MAE: 0.8001, Val Loss: 0.3867, Val MAE Loss: 0.4127\n",
                        "Best model saved at epoch 2\n",
                        "Epoch 3, Train Loss: 0.9354, Train MAE: 0.7120, Val Loss: 0.3604, Val MAE Loss: 0.4066\n",
                        "Best model saved at epoch 3\n",
                        "Epoch 4, Train Loss: 0.6205, Train MAE: 0.5714, Val Loss: 0.6421, Val MAE Loss: 0.5455\n",
                        "Epoch 5, Train Loss: 0.5129, Train MAE: 0.5034, Val Loss: 0.2879, Val MAE Loss: 0.3236\n",
                        "Best model saved at epoch 5\n",
                        "Epoch 6, Train Loss: 0.4262, Train MAE: 0.4668, Val Loss: 0.3174, Val MAE Loss: 0.3786\n",
                        "Epoch 7, Train Loss: 0.2822, Train MAE: 0.3865, Val Loss: 0.3261, Val MAE Loss: 0.3548\n",
                        "Epoch 8, Train Loss: 0.2516, Train MAE: 0.3614, Val Loss: 0.2647, Val MAE Loss: 0.3303\n",
                        "Best model saved at epoch 8\n",
                        "Epoch 9, Train Loss: 0.2357, Train MAE: 0.3511, Val Loss: 0.4760, Val MAE Loss: 0.4424\n",
                        "Epoch 10, Train Loss: 0.2127, Train MAE: 0.3263, Val Loss: 0.3334, Val MAE Loss: 0.3730\n",
                        "Epoch 11, Train Loss: 0.1761, Train MAE: 0.3070, Val Loss: 0.2388, Val MAE Loss: 0.3003\n",
                        "Best model saved at epoch 11\n",
                        "Epoch 12, Train Loss: 0.1560, Train MAE: 0.2889, Val Loss: 0.2157, Val MAE Loss: 0.2808\n",
                        "Best model saved at epoch 12\n",
                        "Epoch 13, Train Loss: 0.1526, Train MAE: 0.2818, Val Loss: 0.2617, Val MAE Loss: 0.3188\n",
                        "Epoch 14, Train Loss: 0.1163, Train MAE: 0.2513, Val Loss: 0.2165, Val MAE Loss: 0.2810\n",
                        "Epoch 15, Train Loss: 0.1182, Train MAE: 0.2533, Val Loss: 0.2487, Val MAE Loss: 0.3238\n",
                        "Early stopping\n"
                    ]
                }
            ],
            "source": [
                "criterion = nn.MSELoss()\n",
                "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
                "MAE = nn.L1Loss()\n",
                "\n",
                "best_val_loss = float('inf')\n",
                "patience = 3\n",
                "counter = 0\n",
                "\n",
                "for epoch in range(25):\n",
                "    model.train()\n",
                "    train_loss = 0.0\n",
                "    mae_train_loss = 0.0\n",
                "\n",
                "    for images, labels in train_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "\n",
                "        optimizer.zero_grad()\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs.squeeze(), labels)\n",
                "        mae_loss = MAE(outputs.squeeze(), labels)\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "\n",
                "        train_loss += loss.item() * images.size(0)\n",
                "        mae_train_loss += mae_loss.item() * images.size(0)\n",
                "\n",
                "    train_loss /= len(train_loader.dataset)\n",
                "    mae_train_loss /= len(train_loader.dataset)\n",
                "\n",
                "    # Validation\n",
                "    model.eval()\n",
                "    val_loss = 0.0\n",
                "    mae_val_loss = 0.0\n",
                "\n",
                "    with torch.no_grad():\n",
                "        for images, labels in valid_loader:\n",
                "            images, labels = images.to(device), labels.to(device)\n",
                "            outputs = model(images)\n",
                "            loss = criterion(outputs.squeeze(), labels)\n",
                "            mae_loss = MAE(outputs.squeeze(), labels)\n",
                "\n",
                "            val_loss += loss.item() * images.size(0)\n",
                "            mae_val_loss += mae_loss.item() * images.size(0)\n",
                "\n",
                "    val_loss /= len(valid_loader.dataset)\n",
                "    mae_val_loss /= len(valid_loader.dataset)\n",
                "\n",
                "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Train MAE: {mae_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val MAE Loss: {mae_val_loss:.4f}\")\n",
                "\n",
                "    if val_loss < best_val_loss:\n",
                "        best_val_loss = val_loss\n",
                "        counter = 0\n",
                "        torch.save(model.state_dict(), f\"best_inception.pth\")\n",
                "        print(f\"Best model saved at epoch {epoch+1}\")\n",
                "    else:\n",
                "        counter += 1\n",
                "        if counter >= patience:\n",
                "            print(\"Early stopping\")\n",
                "            model.load_state_dict(torch.load(f\"best_inception.pth\"))\n",
                "            break"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Loss: 0.1184 - Test MAE Loss: 0.2230\n"
                    ]
                }
            ],
            "source": [
                "model.eval()\n",
                "test_loss = 0.0\n",
                "test_mae_loss = 0.0\n",
                "\n",
                "with torch.no_grad():\n",
                "    for images, labels in test_loader:\n",
                "        images, labels = images.to(device), labels.to(device)\n",
                "        outputs = model(images)\n",
                "        loss = criterion(outputs.squeeze(), labels)\n",
                "        mae = MAE(outputs.squeeze(), labels)\n",
                "\n",
                "        test_loss += loss.item() * images.size(0)\n",
                "        test_mae_loss += mae.item() * images.size(0)\n",
                "\n",
                "test_loss /= len(test_loader.dataset)\n",
                "test_mae_loss /= len(test_loader.dataset)\n",
                "\n",
                "print(f\"Test Loss: {test_loss:.4f} - Test MAE Loss: {test_mae_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def make_prediction(model, img_path):\n",
                "    img = cv2.imread(img_path)\n",
                "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) / 255.0\n",
                "    transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
                "    img = transform(img)\n",
                "    img = img.to(torch.float32)\n",
                "    img = img.to(device)\n",
                "\n",
                "    with torch.no_grad():\n",
                "        outputs = model(img.unsqueeze(0))\n",
                "        return outputs[0][0].item()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "7: 6 - 5.939801216125488\n",
                        "5: 4 - 4.160942077636719\n",
                        "3: 4 - 3.6042773723602295\n",
                        "3: 2 - 2.1351749897003174\n",
                        "4: 3 - 3.1368353366851807\n",
                        "6: 5 - 4.732273101806641\n",
                        "7: 6 - 6.214400291442871\n",
                        "4: 2 - 2.334747314453125\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import confusion_matrix\n",
                "\n",
                "y_true = []\n",
                "y_pred = []\n",
                "\n",
                "with open(dataset_path+\"test/number_of_axles_test.txt\", 'r') as f:\n",
                "    for i in f.readlines():\n",
                "        img_file, label = i.split(\",\")\n",
                "        y_true.append(int(label))\n",
                "\n",
                "        img_file = os.path.join(dataset_path+\"/test/images\", img_file)\n",
                "\n",
                "        no_round = make_prediction(model, img_file)\n",
                "        predicao = round(no_round)\n",
                "        if predicao != int(label):\n",
                "            print(f\"{int(label)}: {predicao} - {no_round}\")\n",
                "        y_pred.append(predicao)\n",
                "\n",
                "confusion_matrix(y_true, y_pred)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "(0.8796660532167778, 0.8519126854233238, 0.8613615476518702, None)\n"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import precision_recall_fscore_support\n",
                "print(precision_recall_fscore_support(y_true, y_pred, average=\"macro\"))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "0.926605504587156"
                    ]
                }
            ],
            "source": [
                "from sklearn.metrics import accuracy_score\n",
                "print(accuracy_score(y_true, y_pred))"
            ]
        }
    ],
    "metadata": {
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
